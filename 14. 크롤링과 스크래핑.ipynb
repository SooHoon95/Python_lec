{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09071116",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ": 인터넷에서 지정된 파일을 내 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2d59e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다 ! \n"
     ]
    }
   ],
   "source": [
    "# library >> 모듈가져오기\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "#URL과 저장경로 지정 (어디서 가져올거니? 어디에 저장할거니?)\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/text.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장 되었습니다 ! \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04129094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703ee68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다..!\n"
     ]
    }
   ],
   "source": [
    "# library >> 모듈가져오기\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "#URL과 저장경로 지정 (어디서 가져올거니? 어디에 저장할거니?)\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/text1.png\"\n",
    "\n",
    "# 다운로드 하기 ( 이번엔 파일로가져오는 게 아니라 메모리에 가져와서 저장할거임)\n",
    "\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기\n",
    "with open(savename, \"wb\") as f: \n",
    "    f.write(mem)\n",
    "print(\"저장 되었습니다..!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96f972",
   "metadata": {},
   "source": [
    "# BeautifulSoup로 Scraping 하기\n",
    "\n",
    ": 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2badacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dkehs\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dkehs\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b51a428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "-------------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1=  Line 1을 서술 \n",
      "p1=  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1>Header</h1>\n",
    "            <p> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"-------------\")\n",
    "\n",
    "# Text만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print (\"p1=\", p1.string)\n",
    "print(\"p1=\", p1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664eb2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\"> Header </h1>\n",
      "<p id=\"body\"> Line 1을 서술 </p>\n",
      "title =  None\n",
      "body =   Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\"> Header </h1>\n",
    "            <p id=\"body\"> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# HTML 분석\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "title = soup.find(id = 'title')\n",
    "body = soup.find(id = 'body')\n",
    "\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# text만 출력\n",
    "print(\"title = \" , title.stirng)\n",
    "print(\"body = \" , body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30ab0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " naver >>> http://www.naver.com\n",
      " daum >>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#HTML Sample\n",
    "\n",
    "html =\"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "                <li><a href = \"http://www.naver.com\"> naver</a></li>\n",
    "                <li><a href = \"http://www.daum.net\"> daum</a></li>\n",
    "                \n",
    "            </ul>\n",
    "    </html\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "links = soup.find_all('a') # 리스트 형식이다.\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links:\n",
    "#     print(link.string)\n",
    "#     print(link.attrs['href'])\n",
    "\n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acaa1a97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-beb67d0cf1d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#     print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mlist_wfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mexcept_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wf' is not defined"
     ]
    }
   ],
   "source": [
    "# 기상청 자료활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# data 가져오기 \n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\") # 링크의 타입은 xml이지만 html.parser도 가져올 수 있다.\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "# title = soup.find('title').string\n",
    "# print(title)\n",
    "# wf = soup.find('wf').string\n",
    "# print(wf)\n",
    "\n",
    "# wf_list = list(wf)\n",
    "# print(wf_list)\n",
    "\n",
    "# for i in wf_list:\n",
    "#     if i == '<':\n",
    "#         continue\n",
    "#     if i == 'b':\n",
    "#         continue\n",
    "#     if i == 'r':\n",
    "#         continue\n",
    "#     if i == '/':\n",
    "#         continue\n",
    "#     if i == '>':\n",
    "#         continue\n",
    "#     if i == '○':\n",
    "#         print()\n",
    "#     if i == '*':\n",
    "#         print()\n",
    "#     print(i, end = '')\n",
    "    \n",
    "# wf_split = wf.split('<br />')\n",
    "# print(wf_split)\n",
    "\n",
    "# for i in wf_split:\n",
    "#     print(i)\n",
    "\n",
    "list_wfs = list(wf)\n",
    "except_char = ['<','b', 'r', '/','>']\n",
    "result = \"\"\n",
    "\n",
    "for lwf in list_wfs:\n",
    "    if lwf not in except_char:\n",
    "        result += lwf\n",
    "print('-' * 100)\n",
    "print(result)\n",
    "type(result)\n",
    "result.split('   ○')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad69306",
   "metadata": {},
   "source": [
    "# CSS 선택자 사용하기\n",
    "\n",
    "soup.select_one() : css 선택자로 요소하나를 추출.   \n",
    "soup.select() : css 선택자로 요소 여러개를 리스트로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id = \"meigen\">\n",
    "                <h1>위키 북스</h1>\n",
    "                <ul class = \"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css로 추출하기\n",
    "# 타이틀 부분 추출하기\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string #  id:#,   class: . ,  > : 자손, \" \" : 후손 \n",
    "print(h1)\n",
    "\n",
    "#목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "# print(li_lists)\n",
    "for li in li_lists:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "# https://finance.naver.com/marketindex/\n",
    "# #exchangeList > li.on > a.head.usd > div > span.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "#데이터 추출하기\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd /  krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미국 , 일본, 유럽연합, 중국의 환율\n",
    "# 일본 : #worldExchangeList > li.on > a.head.jpy_usd > div > span.value\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "#데이터 추출하기\n",
    "prices = soup.select(\"div.head_info > span.value\")\n",
    "print(prices)\n",
    "\n",
    "# for price in prices:\n",
    "#     print(price.string)\n",
    "print(\"미국 : \", prices[0].string)\n",
    "print(\"일본 : \", prices[1].string)\n",
    "print(\"유럽연합 : \", prices[2].string)\n",
    "print(\"중국 : \", prices[3].string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤동주 시안 작품 가져오기\n",
    "# https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\n",
    "#  #mw-content-text > div.mw-parser-output\n",
    "#mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > b > a\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# 분석\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "#데이터 추출하기\n",
    "poems = soup.select(\"#mw-content-text > div.mw-parser-output > ul > li a\")\n",
    "# print(poems)\n",
    "# poem=[]\n",
    "for i in poems:\n",
    "    if i.string == \"증보판\":\n",
    "        continue\n",
    "    print(\"-\", i.string)\n",
    "#     print(i.string, \">>\", \"https://ko.wikisource.org\"+i.attrs['href'])\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4dadd3",
   "metadata": {},
   "source": [
    "# 다음 영화 연간 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://movie.daum.net/ranking/boxoffice/yearly\n",
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "#HTML\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "movies = soup.select(\"#mainContent > div > div.box_boxoffice > ol > li > div > div.thumb_cont > strong > a\")\n",
    "# print(movies)\n",
    "number = 1\n",
    "for i in movies:\n",
    "    print(number,\"순위 : \",i.string)\n",
    "    number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247cbd8",
   "metadata": {},
   "source": [
    "# 다음 IT News 많이 본 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://news.daum.net/digital#1\n",
    "\n",
    "url = \"https://news.daum.net/digital/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "news_lists = soup.select(\"li.on > div a\")\n",
    "#print(news_lists)\n",
    "\n",
    "for news in news_lists:\n",
    "    print(news.attrs['href'], \":\", news.string.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199113b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296cfff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
